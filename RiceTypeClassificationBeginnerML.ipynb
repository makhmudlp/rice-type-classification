{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "8tkNywdQaZ0F",
        "Ol6kTN36YRFZ",
        "xKJRkB0iYtla",
        "xcAX-sK64xcg",
        "LH4LKmI36kdi",
        "SqncXcoWZgn8",
        "I_HHh5gpZpEU",
        "-DobnjQfZvlf",
        "NE3H64KxFQ8h",
        "KLoHfDevJEjx",
        "6S2RwXCkZ5TG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Rice Type Classification\n"
      ],
      "metadata": {
        "id": "qTZhYvSlEZAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting our dataset from kaggle\n"
      ],
      "metadata": {
        "id": "8tkNywdQaZ0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --quiet\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/mssmartypants/rice-type-classification\")"
      ],
      "metadata": {
        "id": "C049OHwhEbw3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing libraries and choosing device"
      ],
      "metadata": {
        "id": "Ol6kTN36YRFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "Puwi36HrFqKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing and Normalization\n",
        "\n",
        "Normalization:\n",
        "we are making each columns' maximum value to 1:\n",
        "take all the values from each column devide by the largest for each"
      ],
      "metadata": {
        "id": "xKJRkB0iYtla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(\"/content/rice-type-classification/riceClassification.csv\")\n",
        "data_df.dropna(inplace=True)# 'inplace = True' drops any missing value\n",
        "data_df.drop(['id'], axis=1, inplace=True) #axis=1 means it is column\n",
        "\n",
        "original_df=data_df.copy()\n",
        "for column in data_df.columns:\n",
        "  data_df[column]=data_df[column]/data_df[column].abs().max()"
      ],
      "metadata": {
        "id": "nZRQUB7FEA_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and Test splitting\n",
        "\n"
      ],
      "metadata": {
        "id": "xcAX-sK64xcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array(data_df.iloc[:,:-1]) #iloc means specific columns\n",
        "Y=np.array(data_df.iloc[:,-1]) #we are taking only the last column\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)\n"
      ],
      "metadata": {
        "id": "OC2z3KOQ4w07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Dataset Class\n",
        "\n",
        "We want to convert our data to the one that PyTorch understands.\n",
        "\n",
        "PyTorch understands tensor.\n"
      ],
      "metadata": {
        "id": "LH4LKmI36kdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, X, Y):\n",
        "    self.X=torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    self.Y=torch.tensor(Y, dtype=torch.float32).to(device)\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]"
      ],
      "metadata": {
        "id": "eU3ZKgzA6jxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=dataset(X_train, y_train)\n",
        "validation_data=dataset(X_val, y_val)\n",
        "testing_data=dataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "qAWcYQZN8DJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader=DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "val_dataloader=DataLoader(validation_data, batch_size=32, shuffle=True)\n",
        "test_dataloader=DataLoader(testing_data, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "uOxOsyJP9A1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Module"
      ],
      "metadata": {
        "id": "SqncXcoWZgn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.input_layer = nn.Linear(X.shape[1], 10)\n",
        "    self.linear = nn.Linear(10, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.input_layer(x)\n",
        "    x=self.linear(x)\n",
        "    x=self.sigmoid(x)\n",
        "    return x\n",
        "model=MyModel().to(device)"
      ],
      "metadata": {
        "id": "SHEDkEQF9sVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can check the summary of our Model"
      ],
      "metadata": {
        "id": "I_HHh5gpZpEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (X.shape[1],))"
      ],
      "metadata": {
        "id": "TstkDdnpAIEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating loss function and optimizer"
      ],
      "metadata": {
        "id": "-DobnjQfZvlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "GKyXNbQoAuym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop"
      ],
      "metadata": {
        "id": "NE3H64KxFQ8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss_train_plot = []\n",
        "total_loss_validation_plot = []\n",
        "total_acc_train_plot = []\n",
        "total_acc_validation_plot = []\n",
        "\n",
        "epochs=10\n",
        "for epoch in range(epochs):\n",
        "  total_acc_train=0\n",
        "  total_loss_train=0\n",
        "  total_acc_val=0\n",
        "  total_loss_val=0\n",
        "  for data in train_dataloader:\n",
        "    x, y = data\n",
        "    prediction=model(x).squeeze(1)\n",
        "    batch_loss=criterion(prediction, y)\n",
        "    total_loss_train+=batch_loss.item()\n",
        "    acc=(prediction.round()==y).sum().item()\n",
        "    total_acc_train+=acc\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  with torch.no_grad():\n",
        "    for data in val_dataloader:\n",
        "      x, y= data\n",
        "      prediction=model(x).squeeze(1)\n",
        "      batch_loss=criterion(prediction, y)\n",
        "      total_loss_val+=batch_loss.item()\n",
        "      acc=(prediction.round()==y).sum().item()\n",
        "      total_acc_val+=acc\n",
        "  total_loss_train_plot.append(round(total_loss_train/1000, 4))\n",
        "  total_loss_validation_plot.append(round(total_loss_val/1000,4))\n",
        "\n",
        "  total_acc_train_plot.append(round(total_acc_train/training_data.__len__()*100,4))\n",
        "  total_acc_validation_plot.append(round(total_acc_val/validation_data.__len__()*100,4))\n",
        "  print(f'''Epoch: {epoch+1} Train Loss {round(total_loss_train/1000, 4)} Train Acc:{round(total_acc_train/training_data.__len__()*100,4)}\n",
        "        Validation Loss: {round(total_loss_val/1000,4)}''')\n",
        "  print(\"=\"*25)"
      ],
      "metadata": {
        "id": "-w9YmBMrBKnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing Loop"
      ],
      "metadata": {
        "id": "KLoHfDevJEjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  total_acc_test=0\n",
        "  total_loss_test=0\n",
        "  for data in test_dataloader:\n",
        "    x, y = data\n",
        "\n",
        "    prediction = model(x).squeeze(1)\n",
        "    batch_loss_test=criterion(prediction, y)\n",
        "    total_loss_test+=batch_loss_test\n",
        "    acc=(prediction.round()==y).sum().item()\n",
        "    total_acc_test+=acc\n",
        "  print(f\"Accuracy: {round(total_acc_test/testing_data.__len__()*100,4)}\")"
      ],
      "metadata": {
        "id": "ERxaNPdJJEUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting our results"
      ],
      "metadata": {
        "id": "6S2RwXCkZ5TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
        "axs[0].plot(total_loss_train_plot, label=\"Train Loss\")\n",
        "axs[0].plot(total_loss_validation_plot, label=\"Validation Loss\")\n",
        "axs[0].set_title(\"Training and Validation Loss over Epochs\")\n",
        "axs[0].set_xlabel('Epochs')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_ylim(0,1)\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(total_acc_train_plot, label=\"Train Accuracy\")\n",
        "axs[1].plot(total_acc_validation_plot, label=\"Validation Accuracy\")\n",
        "axs[1].set_title(\"Training and Validation Accuracy over Epochs\")\n",
        "axs[1].set_xlabel('Epochs')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_ylim(0,100)\n",
        "axs[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f3TNRNCDKZa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How our Model predicsts? This is sample input and how to get the prediction. The numbers and input is just random.\n"
      ],
      "metadata": {
        "id": "g_1U2K3LZ88L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "area=2353/original_df['Area'].abs().max()\n",
        "MajorAxis=81/original_df['MajorAxisLength'].abs().max()\n",
        "MinorAxis=43/original_df['MinorAxisLength'].abs().max()\n",
        "Eccentricity=43/original_df['Eccentricity'].abs().max()\n",
        "ConvexArea=35/original_df['ConvexArea'].abs().max()\n",
        "EquivDiameter=45/original_df['EquivDiameter'].abs().max()\n",
        "Extent=34/original_df['Extent'].abs().max()\n",
        "Perimeter=567/original_df['Perimeter'].abs().max()\n",
        "Roundness=56/original_df['Roundness'].abs().max()\n",
        "AspectRation=2/original_df['AspectRation'].abs().max()\n",
        "my_prediction=model(torch.tensor([area,MajorAxis,MinorAxis,Eccentricity,ConvexArea, EquivDiameter,Extent,Perimeter,Roundness, AspectRation], dtype=torch.float32).to(device))\n",
        "round(my_prediction.item())"
      ],
      "metadata": {
        "id": "312htMS_Mmlu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}